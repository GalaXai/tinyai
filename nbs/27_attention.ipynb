{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68083308",
   "metadata": {},
   "source": [
    "# Attention from scratch using tinyai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273a6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,torch\n",
    "from torch import nn\n",
    "from tinyai.activations import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bd330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "#   dimentions (batch_size,channels,height,width) # BCHW\n",
    "x = torch.randn(64,32,16,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd762446",
   "metadata": {},
   "source": [
    "In this step, we reshape the 'image' into a single tensor. Additionally, since it is originally designed for NLP tasks, the sequence (height x width) comes before the channels.\\\n",
    "Therefore, we also perform a transpose operation here to rearrange the dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8174db82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51245dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f10cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15237a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d34cd0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]),\n",
       " torch.Size([64, 256, 32]),\n",
       " torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4259a6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e421d1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9da52b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "view(*shape) -> Tensor\n",
      "\n",
      "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "different :attr:`shape`.\n",
      "\n",
      "The returned tensor shares the same data and must have the same number\n",
      "of elements, but may have a different size. For a tensor to be viewed, the new\n",
      "view size must be compatible with its original size and stride, i.e., each new\n",
      "view dimension must either be a subspace of an original dimension, or only span\n",
      "across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      "contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      "\n",
      ".. math::\n",
      "\n",
      "  \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      "\n",
      "Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      "without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      ":meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      "returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      ":meth:`contiguous`) otherwise.\n",
      "\n",
      "Args:\n",
      "    shape (torch.Size or int...): the desired size\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(4, 4)\n",
      "    >>> x.size()\n",
      "    torch.Size([4, 4])\n",
      "    >>> y = x.view(16)\n",
      "    >>> y.size()\n",
      "    torch.Size([16])\n",
      "    >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      "    >>> z.size()\n",
      "    torch.Size([2, 8])\n",
      "\n",
      "    >>> a = torch.randn(1, 2, 3, 4)\n",
      "    >>> a.size()\n",
      "    torch.Size([1, 2, 3, 4])\n",
      "    >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      "    >>> b.size()\n",
      "    torch.Size([1, 3, 2, 4])\n",
      "    >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      "    >>> c.size()\n",
      "    torch.Size([1, 3, 2, 4])\n",
      "    >>> torch.equal(b, c)\n",
      "    False\n",
      "\n",
      "\n",
      ".. method:: view(dtype) -> Tensor\n",
      "   :noindex:\n",
      "\n",
      "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "different :attr:`dtype`.\n",
      "\n",
      "If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
      "then the size of the last dimension of the output will be scaled\n",
      "proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
      "``self.dtype``, then each pair of elements in the last dimension of\n",
      ":attr:`self` will be combined, and the size of the last dimension of the output\n",
      "will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
      "of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
      "be split in two, and the size of the last dimension of the output will be\n",
      "double that of :attr:`self`. For this to be possible, the following conditions\n",
      "must be true:\n",
      "\n",
      "    * ``self.dim()`` must be greater than 0.\n",
      "    * ``self.stride(-1)`` must be 1.\n",
      "\n",
      "Additionally, if the element size of :attr:`dtype` is greater than that of\n",
      "``self.dtype``, the following conditions must be true as well:\n",
      "\n",
      "    * ``self.size(-1)`` must be divisible by the ratio between the element\n",
      "      sizes of the dtypes.\n",
      "    * ``self.storage_offset()`` must be divisible by the ratio between the\n",
      "      element sizes of the dtypes.\n",
      "    * The strides of all dimensions, except the last dimension, must be\n",
      "      divisible by the ratio between the element sizes of the dtypes.\n",
      "\n",
      "If any of the above conditions are not met, an error is thrown.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "    This overload is not supported by TorchScript, and using it in a Torchscript\n",
      "    program will cause undefined behavior.\n",
      "\n",
      "\n",
      "Args:\n",
      "    dtype (:class:`torch.dtype`): the desired dtype\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(4, 4)\n",
      "    >>> x\n",
      "    tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      "            [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "            [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "            [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "    >>> x.dtype\n",
      "    torch.float32\n",
      "\n",
      "    >>> y = x.view(torch.int32)\n",
      "    >>> y\n",
      "    tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      "            [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      "            [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      "            [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      "        dtype=torch.int32)\n",
      "    >>> y[0, 0] = 1000000000\n",
      "    >>> x\n",
      "    tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      "            [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "            [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "            [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "\n",
      "    >>> x.view(torch.cfloat)\n",
      "    tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
      "            [-0.1520+0.7472j,  0.5617-0.8649j],\n",
      "            [-2.4724-0.0334j, -0.2976-0.8499j],\n",
      "            [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
      "    >>> x.view(torch.cfloat).size()\n",
      "    torch.Size([4, 2])\n",
      "\n",
      "    >>> x.view(torch.uint8)\n",
      "    tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
      "               8, 191],\n",
      "            [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
      "              93, 191],\n",
      "            [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
      "              89, 191],\n",
      "            [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
      "              28, 191]], dtype=torch.uint8)\n",
      "    >>> x.view(torch.uint8).size()\n",
      "    torch.Size([4, 16])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "print(x.view(64,32,-1).shape)\n",
    "t.view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15062786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.GroupNorm(1, ni)\n",
    "        self.q = nn.Linear(ni, ni)\n",
    "        self.k = nn.Linear(ni, ni)\n",
    "        self.v = nn.Linear(ni, ni)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        s = (q@k.transpose(1,2)) / self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcb48706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fb4ae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce69830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1a380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_parms(a,b):\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f11b58",
   "metadata": {},
   "source": [
    "The code in the cell below is copied from an old version of Hugging Face's repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddeb85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        num_head_channels= None,\n",
    "        norm_num_groups: int = 32,\n",
    "        rescale_output_factor: float = 1.0,\n",
    "        eps: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.num_heads = channels // num_head_channels if num_head_channels is not None else 1\n",
    "        self.num_head_size = num_head_channels\n",
    "        self.group_norm = nn.GroupNorm(num_channels=channels, num_groups=norm_num_groups, eps=eps, affine=True)\n",
    "\n",
    "        # define q,k,v as linear layers\n",
    "        self.query = nn.Linear(channels, channels)\n",
    "        self.key = nn.Linear(channels, channels)\n",
    "        self.value = nn.Linear(channels, channels)\n",
    "\n",
    "        self.rescale_output_factor = rescale_output_factor\n",
    "        self.proj_attn = nn.Linear(channels, channels, 1)\n",
    "\n",
    "        self._use_memory_efficient_attention_xformers = False\n",
    "        self._attention_op = None\n",
    "\n",
    "    def reshape_heads_to_batch_dim(self, tensor):\n",
    "        batch_size, seq_len, dim = tensor.shape\n",
    "        head_size = self.num_heads\n",
    "        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
    "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
    "        return tensor\n",
    "\n",
    "    def reshape_batch_dim_to_heads(self, tensor):\n",
    "        batch_size, seq_len, dim = tensor.shape\n",
    "        head_size = self.num_heads\n",
    "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
    "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        batch, channel, height, width = hidden_states.shape\n",
    "\n",
    "        # norm\n",
    "        hidden_states = self.group_norm(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        # proj to q, k, v\n",
    "        query_proj = self.query(hidden_states)\n",
    "        key_proj = self.key(hidden_states)\n",
    "        value_proj = self.value(hidden_states)\n",
    "\n",
    "        scale = 1 / math.sqrt(self.channels / self.num_heads)\n",
    "\n",
    "        query_proj = self.reshape_heads_to_batch_dim(query_proj)\n",
    "        key_proj = self.reshape_heads_to_batch_dim(key_proj)\n",
    "        value_proj = self.reshape_heads_to_batch_dim(value_proj)\n",
    "\n",
    "\n",
    "        attention_scores = torch.baddbmm(\n",
    "            torch.empty(\n",
    "                query_proj.shape[0],\n",
    "                query_proj.shape[1],\n",
    "                key_proj.shape[1],\n",
    "                dtype=query_proj.dtype,\n",
    "                device=query_proj.device,\n",
    "            ),\n",
    "            query_proj,\n",
    "            key_proj.transpose(-1, -2),\n",
    "            beta=0,\n",
    "            alpha=scale,\n",
    "        )\n",
    "        attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores.dtype)\n",
    "        hidden_states = torch.bmm(attention_probs, value_proj)\n",
    "\n",
    "        # reshape hidden_states\n",
    "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
    "\n",
    "        # compute next hidden_states\n",
    "        hidden_states = self.proj_attn(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n",
    "\n",
    "        # res connect and rescale\n",
    "        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc6e3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = AttentionBlock(32, norm_num_groups=1)\n",
    "src = sa.q,sa.k,sa.v,sa.proj,sa.norm\n",
    "dst = at.query,at.key,at.value,at.proj_attn,at.group_norm\n",
    "for s,d in zip(src,dst): cp_parms(s,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf2afb",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4ac6751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d5e25",
   "metadata": {},
   "source": [
    "# Diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bfc0087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = at(x)\n",
    "rb[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a4f25e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 96])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqkv = nn.Linear(ni, ni*3)\n",
    "st = sqkv(t)\n",
    "st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64df0786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = torch.chunk(st, 3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afdd291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k@q.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde31928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67202ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eadf0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0085, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a167b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_to_batch(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1,sl,d*heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11734bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c3466d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = rearrange(t , 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64105f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f34fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape,t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4739d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ced6c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale = math.sqrt(ni/nheads)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        \n",
    "        # nhead * the bath size but, channels  /  nheads \n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        \n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        # chunk since we  are using ni*3 for qkv\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3ed8798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4)\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18c46b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0093, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0066, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.mean(),sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd9b3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx,nmw = nm(t,t,t)\n",
    "nmx = nmx+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "451f2d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0015, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0050, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmx.mean(),nmx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078e184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d759176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-tinykernel",
   "language": "python",
   "name": "my-python3-tinykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
